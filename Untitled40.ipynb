{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmssMZr/6NN2786RwfvBP5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santoshitgithub/b.github.io/blob/main/Untitled40.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "56knVj1p3PJ3"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch.nn.functional as f\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
        "    temp = query.bmm(key.transpose(1, 2))\n",
        "    scale = query.size(-1) ** 0.5\n",
        "    softmax = f.softmax(temp / scale, dim=-1)\n",
        "    return softmax.bmm(value)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, dim_in: int, dim_q: int, dim_k: int):\n",
        "        super().__init__()\n",
        "        self.q = nn.Linear(dim_in, dim_q)\n",
        "        self.k = nn.Linear(dim_in, dim_k)\n",
        "        self.v = nn.Linear(dim_in, dim_k)\n",
        "\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
        "        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))"
      ],
      "metadata": {
        "id": "WdlXw9Wq3dSi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads: int, dim_in: int, dim_q: int, dim_k: int):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [AttentionHead(dim_in, dim_q, dim_k) for _ in range(num_heads)]\n",
        "        )\n",
        "        self.linear = nn.Linear(num_heads * dim_k, dim_in)\n",
        "\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
        "        return self.linear(\n",
        "            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n",
        "        )"
      ],
      "metadata": {
        "id": "mp3aZO3Z3jEw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def position_encoding(\n",
        "    seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\"),\n",
        ") -> Tensor:\n",
        "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
        "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
        "    phase = pos / (1e4 ** (dim / dim_model))\n",
        "\n",
        "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
      ],
      "metadata": {
        "id": "OgAkpLWJ3ofG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward(dim_input: int = 512, dim_feedforward: int = 2048) -> nn.Module:\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(dim_input, dim_feedforward),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(dim_feedforward, dim_input),\n",
        "    )"
      ],
      "metadata": {
        "id": "2RZywLC-3uZu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.sublayer = sublayer\n",
        "        self.norm = nn.LayerNorm(dimension)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, *tensors: Tensor) -> Tensor:\n",
        "        # Assume that the \"query\" tensor is given first, so we can compute the\n",
        "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
        "        return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))"
      ],
      "metadata": {
        "id": "q23XGWG_30PF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim_model: int = 512,\n",
        "        num_heads: int = 6,\n",
        "        dim_feedforward: int = 2048,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
        "        self.attention = Residual(\n",
        "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
        "            dimension=dim_model,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.feed_forward = Residual(\n",
        "            feed_forward(dim_model, dim_feedforward),\n",
        "            dimension=dim_model,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "    def forward(self, src: Tensor) -> Tensor:\n",
        "        src = self.attention(src, src, src)\n",
        "        return self.feed_forward(src)\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers: int = 6,\n",
        "        dim_model: int = 512,\n",
        "        num_heads: int = 8,\n",
        "        dim_feedforward: int = 2048,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, src: Tensor) -> Tensor:\n",
        "        seq_len, dimension = src.size(1), src.size(2)\n",
        "        src += position_encoding(seq_len, dimension)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src)\n",
        "\n",
        "        return src"
      ],
      "metadata": {
        "id": "oROa5gVB35n0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim_model: int = 512,\n",
        "        num_heads: int = 6,\n",
        "        dim_feedforward: int = 2048,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
        "        self.attention_1 = Residual(\n",
        "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
        "            dimension=dim_model,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.attention_2 = Residual(\n",
        "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
        "            dimension=dim_model,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.feed_forward = Residual(\n",
        "            feed_forward(dim_model, dim_feedforward),\n",
        "            dimension=dim_model,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
        "        tgt = self.attention_1(tgt, tgt, tgt)\n",
        "        tgt = self.attention_2(tgt, memory, memory)\n",
        "        return self.feed_forward(tgt)\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers: int = 6,\n",
        "        dim_model: int = 512,\n",
        "        num_heads: int = 8,\n",
        "        dim_feedforward: int = 2048,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.linear = nn.Linear(dim_model, dim_model)\n",
        "\n",
        "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
        "        seq_len, dimension = tgt.size(1), tgt.size(2)\n",
        "        tgt += position_encoding(seq_len, dimension)\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, memory)\n",
        "\n",
        "        return torch.softmax(self.linear(tgt), dim=-1)"
      ],
      "metadata": {
        "id": "yYVtkddz4GGR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        num_encoder_layers: int = 6,\n",
        "        num_decoder_layers: int = 6,\n",
        "        dim_model: int = 512, \n",
        "        num_heads: int = 6, \n",
        "        dim_feedforward: int = 2048, \n",
        "        dropout: float = 0.1, \n",
        "        activation: nn.Module = nn.ReLU(),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = TransformerEncoder(\n",
        "            num_layers=num_encoder_layers,\n",
        "            dim_model=dim_model,\n",
        "            num_heads=num_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.decoder = TransformerDecoder(\n",
        "            num_layers=num_decoder_layers,\n",
        "            dim_model=dim_model,\n",
        "            num_heads=num_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n",
        "        return self.decoder(tgt, self.encoder(src))"
      ],
      "metadata": {
        "id": "eEPRzL-j4OCW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src = torch.rand(64, 32, 512)\n",
        "tgt = torch.rand(64, 16, 512)\n",
        "out = Transformer()(src, tgt)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je2zyIaI4Wkj",
        "outputId": "7d8b0766-f30c-4805-f114-25fcba696b39"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 16, 512])\n"
          ]
        }
      ]
    }
  ]
}